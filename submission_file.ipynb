{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting .csv file to a pandas df\n",
    "df = pd.read_csv(\"../csv_files/development.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Description of the data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Histogram of the 5 features on the first pad\n",
    "pad_1 = df[[\"pmax[1]\", \"negpmax[1]\", \"area[1]\", \"tmax[1]\", \"rms[1]\"]]\n",
    "pad_1.hist(bins=100, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Correlation matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "correlation_matrix = df.corr()\n",
    "plt.figure(figsize=(20,20))\n",
    "sns.heatmap(correlation_matrix, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there are some pads/features that has low/none correlation with x and y values. Seems like it is pad (0, 7, 12, 15, 16, 17).  \n",
    "Rms feature has also low correlation with x and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing pads with format: pads = [\"0\", \"7\", \"12\", ..]\n",
    "def drop_pads(df, pads):\n",
    "    cols_to_drop = [col for col in df.columns if any(idx in col for idx in pads)]\n",
    "    df_removed = df.drop(cols_to_drop, axis=1)    \n",
    "    return df_removed\n",
    "\n",
    "df_copy = df \n",
    "df_copy.drop([\"x\", \"y\"], axis=1, inplace=True)\n",
    "pads = [\"0\", \"7\", \"12\", \"15\", \"16\", \"17\"]\n",
    "\n",
    "df_without_noise = drop_pads(df_copy,pads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualization without pads (0, 7, 12, 15, 16, 17)\n",
    "correlation_matrix = df_without_noise.corr()\n",
    "plt.figure(figsize=(20,20))\n",
    "sns.heatmap(correlation_matrix, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile2(dframe, lw=0.05, up=0.95, drop=True):\n",
    "    tresholds = {}\n",
    "    for col_name in dframe.columns:\n",
    "        lw_tresh = dframe[col_name].quantile(lw)\n",
    "        up_tresh = dframe[col_name].quantile(up)\n",
    "        tresholds[col_name] = [lw_tresh, up_tresh]\n",
    "    print(f\"tresholds for {lw}, {up}: {tresholds}\")\n",
    "    initial_dim = dframe.shape\n",
    "    for col_name in dframe.columns:\n",
    "        if drop:\n",
    "            dframe.drop(dframe[dframe[col_name] < tresholds[col_name][0]].index, inplace=True)\n",
    "            dframe.drop(dframe[dframe[col_name] > tresholds[col_name][1]].index, inplace=True)\n",
    "        else:\n",
    "            dframe.loc[df[col_name] < tresholds[col_name][0], col_name] = tresholds[col_name][0]\n",
    "            dframe.loc[df[col_name] > tresholds[col_name][1], col_name] = tresholds[col_name][1]\n",
    "\n",
    "    new_dim = dframe.shape\n",
    "    print(f\"\"\"\n",
    "          initial dim:   {initial_dim}\n",
    "          new dim:       {new_dim}\n",
    "          a reduction of {((initial_dim[0]-new_dim[0])/initial_dim[0])*100}% of rows\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code for extracting most correlated features\n",
    "def get_redundant_pairs(df):\n",
    "    pairs_to_drop = set()\n",
    "    cols = df.columns\n",
    "    for i in range(0, df.shape[1]):\n",
    "        for j in range(0, i+1):\n",
    "            pairs_to_drop.add((cols[i], cols[j]))\n",
    "    return pairs_to_drop\n",
    "\n",
    "def get_top_abs_correlations(df, n=5):\n",
    "    au_corr = df.corr().abs().unstack()\n",
    "    labels_to_drop = get_redundant_pairs(df)\n",
    "    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n",
    "    return au_corr[0:n]\n",
    "\n",
    "print(\"Top Absolute Correlations\")\n",
    "print(get_top_abs_correlations(df, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the evaluation data\n",
    "dev = pd.read_csv(\"../csv_files/development.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the positions and removes the x and y column.\n",
    "import numpy as np\n",
    "pos_dev = dev[[\"x\", \"y\"]]\n",
    "\n",
    "## Dropping data from x and y \n",
    "dev = dev.drop([\"x\", \"y\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing pads with format: pads = [0, 7, 12, ..]\n",
    "def drop_pads(input_list, df):\n",
    "    for i in input_list:\n",
    "        columns_to_remove = df.filter(like=f'[{i}]').columns\n",
    "        df = df.drop(columns=columns_to_remove)\n",
    "    return df\n",
    "\n",
    "remove_pads = [0, 7, 12, 15, 16, 17]\n",
    "dev_interesting_data = drop_pads(remove_pads, dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing features rms\n",
    "def drop_rms_features(df):\n",
    "    # Extract columns that start with 'rms'\n",
    "    rms_columns = [col for col in df.columns if not col.startswith('rms')]\n",
    "\n",
    "    # Create a new DataFrame without 'rms' columns\n",
    "    df_without_rms = df[rms_columns] \n",
    "    return df_without_rms\n",
    "\n",
    "dev_interesting_data = drop_rms_features(dev_interesting_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing tmax feature\n",
    "def drop_tmax_features(df):\n",
    "    # Extract columns that start with 'rms'\n",
    "    tmax_columns = [col for col in df.columns if not col.startswith('tmax')]\n",
    "\n",
    "    # Create a new DataFrame without 'rms' columns\n",
    "    df_without_tmax = df[tmax_columns] \n",
    "    return df_without_tmax\n",
    "\n",
    "dev_interesting_data = drop_tmax_features(dev_interesting_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Z-transformation of the data. Remember to scale accordingly to training data for eval data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(dev_interesting_data)\n",
    " \n",
    "dev_interesting_data = pd.DataFrame(scaler.transform(dev_interesting_data), columns=dev_interesting_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reducing the dataset to X percent of original size to speed up model testing\n",
    "dev_interesting_data_sample = dev_interesting_data.sample(frac=0.20)\n",
    "pos_dev_sample = pos_dev.loc[dev_interesting_data_sample.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting into train and validation set\n",
    "X_train, X_val, pos_train, pos_val = train_test_split(dev_interesting_data_sample, pos_dev_sample, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code for analysing x and y data seperately\n",
    "x_train_data = []\n",
    "y_train_data = []\n",
    "for i in range(len(pos_train)):\n",
    "    x_train_data.append(pos_train.values[i][0])\n",
    "    y_train_data.append(pos_train.values[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import math\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "## NOTE: Lines that are commented out are models and parameter that have been tried during the development.\n",
    "\n",
    "# numb_trees = 70\n",
    "# base_regressor = RandomForestRegressor(n_estimators=numb_trees, criterion=\"poisson\", max_depth=30, max_features=0.35, bootstrap=True) \n",
    "# base_regressor = GradientBoostingRegressor() \n",
    "base_regressor = MLPRegressor(activation=\"logistic\", alpha=0.0001, hidden_layer_sizes=(100, 100), learning_rate='constant', learning_rate_init=0.001, max_iter=1000, solver='adam', tol=0.001)\n",
    "mult_regr = MultiOutputRegressor(base_regressor)\n",
    "\n",
    "## Param grid for GridSearchCV for the MLPRegressor model\n",
    "# param_grid = {\n",
    "#     'hidden_layer_sizes': [(100,), (100,100), (100, 100, 100)],\n",
    "#     'activation': ['relu'],\n",
    "#     'solver': ['adam'],\n",
    "#     'alpha': [0.1],\n",
    "#     'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "#     'max_iter': [500, 1000, 2000],\n",
    "#     'learning_rate_init': [0.001],\n",
    "#     'tol': [0.001, 0.0001]\n",
    "# }\n",
    "\n",
    "# grid_search = GridSearchCV(estimator=base_regressor, param_grid=param_grid, n_jobs=-1, cv=3, scoring=\"neg_mean_absolute_error\")\n",
    "\n",
    "# # Fit the GridSearchCV on the x data\n",
    "# grid_search.fit(X_train, x_train_data)\n",
    "\n",
    "# # Best parameters and best score\n",
    "# best_params = grid_search.best_params_\n",
    "# best_score = grid_search.best_score_\n",
    "\n",
    "# print(\"Best Parameters:\", best_params)\n",
    "# print(\"Best Score:\", best_score)\n",
    "\n",
    "\n",
    "mult_regr.fit(X_train, pos_train)\n",
    "# mult_regr.fit(dev_interesting_data, pos_dev)\n",
    "pos_pred = mult_regr.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Plotting the feature importance for x and y-coordinate with the RandomForestRegressor using mean decrease in impurity. \n",
    "# importances_x = mult_regr_x.feature_importances_\n",
    "# std = np.std([tree.feature_importances_ for tree in mult_regr_x.estimators_], axis=0)\n",
    "\n",
    "# forest_importances_x = pd.Series(importances_x, index=dev_interesting_data_sample.columns)\n",
    "# fig, ax = plt.subplots()\n",
    "# forest_importances_x.plot.bar(yerr=std, ax=ax)\n",
    "# ax.set_title(\"Feature importances for x-coordinate using MDI\")\n",
    "# ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "# fig.tight_layout()\n",
    "\n",
    "# importances_y = mult_regr_y.feature_importances_\n",
    "# std = np.std([tree.feature_importances_ for tree in mult_regr_y.estimators_], axis=0)\n",
    "\n",
    "# forest_importances_y = pd.Series(importances_y, index=dev_interesting_data_sample.columns)\n",
    "# fig, ax = plt.subplots()\n",
    "# forest_importances_y.plot.bar(yerr=std, ax=ax)\n",
    "# ax.set_title(\"Feature importances for y-coordinate using MDI\")\n",
    "# ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "# fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Plotting the feature by importance in decreasing order.\n",
    "# # Convert the importances into a DataFrame\n",
    "# feature_importances_x = pd.DataFrame({\"feature\": X_train.columns, \"importance\": importances_x})\n",
    "\n",
    "# # Sort the DataFrame by importance\n",
    "# feature_importances_x = feature_importances_x.sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# # Plotting\n",
    "# plt.figure(figsize=(10,6))\n",
    "# plt.title(\"Feature Importances for x-coordinate\")\n",
    "# plt.bar(feature_importances_x[\"feature\"], feature_importances_x[\"importance\"], color=\"b\")\n",
    "# plt.xlabel(\"Features\")\n",
    "# plt.ylabel(\"Importance\")\n",
    "# plt.xticks(rotation=\"vertical\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# # Convert the importances into a DataFrame\n",
    "# feature_importances_y = pd.DataFrame({\"feature\": X_train.columns, \"importance\": importances_y})\n",
    "\n",
    "# # Sort the DataFrame by importance\n",
    "# feature_importances_y = feature_importances_y.sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# # Plotting\n",
    "# plt.figure(figsize=(10,6))\n",
    "# plt.title(\"Feature Importances for y-coordinate\")\n",
    "# plt.bar(feature_importances_y[\"feature\"], feature_importances_y[\"importance\"], color=\"b\")\n",
    "# plt.xlabel(\"Features\")\n",
    "# plt.ylabel(\"Importance\")\n",
    "# plt.xticks(rotation=\"vertical\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics to evaluate model \n",
    "import sklearn.metrics as sm\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def avg_euc_dist(pos_val, pos_pred):\n",
    "    sum_square = 0\n",
    "    for i in range(len(pos_val)):\n",
    "        sum_square += math.sqrt((pos_val[i][0]-pos_pred[i][0])**2 + (pos_val[i][1]-pos_pred[i][1])**2)\n",
    "    return sum_square/len(pos_val)    \n",
    "\n",
    "def metrics_on_model(pos_val, pos_pred):\n",
    "    print(\"Mean absolute error =\", round(sm.mean_absolute_error(pos_val, pos_pred), 2)) \n",
    "    print(\"Mean squared error =\", round(sm.mean_squared_error(pos_val, pos_pred), 2)) \n",
    "    print(\"Median absolute error =\", round(sm.median_absolute_error(pos_val, pos_pred), 2)) \n",
    "    print(\"Explain variance score =\", round(sm.explained_variance_score(pos_val, pos_pred), 2)) \n",
    "    print(\"R2 score =\", round(sm.r2_score(pos_val, pos_pred), 2))\n",
    "    print(\"Mean eucledian distance =\", round(avg_euc_dist(pos_val, pos_pred), 2))\n",
    "\n",
    "metrics_on_model(pos_val.to_numpy(), pos_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model on the evaluation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ev_data = pd.read_csv(\"../csv_files/evaluation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extracting the ID\n",
    "# eval_id = ev_data[\"Id\"]\n",
    "\n",
    "# # Dropping the Id column from the ev_data\n",
    "# ev_data = ev_data.drop([\"Id\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Formatting the position array to a string to be used in the .csv file \n",
    "# def pred_to_string(prediction_array):\n",
    "#     pred_column = []\n",
    "#     for i in range(len(prediction_array)):\n",
    "#         pos_string = (str(prediction_array[i][0]) + \"|\" + str(prediction_array[i][1]))\n",
    "#         pred_column.append(pos_string)\n",
    "#     return pred_column\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Preprocessing \n",
    "# remove_pads = [0, 7, 12, 15, 16, 17]\n",
    "# ev_data = drop_pads(remove_pads, ev_data) # Remove pads\n",
    "# ev_data = drop_rms_features(ev_data) # Remove rms feature \n",
    "# ev_data = drop_tmax_features(ev_data) # Remove t_max feature\n",
    "# ev_data = pd.DataFrame(scaler.transform(ev_data), columns=ev_data.columns) # Z-transform with mean and std from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predicting the evaluation results\n",
    "# mult_regr_eval = mult_regr.predict(ev_data)\n",
    "# pos_pred = pred_to_string(mult_regr_eval) # Formatting the predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating a df and .csv file to be submitted. Saved in submission_file folder\n",
    "# mult_reg_submission = pd.DataFrame({'Id': eval_id, 'Predicted': pos_pred})\n",
    "# mult_reg_submission.to_csv(\"../DataScienceLab_Project/submission_files/MLP_all_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
